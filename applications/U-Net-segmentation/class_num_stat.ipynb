{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_COLORS = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "\n",
    "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n",
    "\n",
    "COLORMAP = dict(zip([tuple(lst) for lst in VOC_COLORS], range(21)))\n",
    "COLORMAP[(224, 224, 192)] = 0\n",
    "COLORMAP\n",
    "\n",
    "\n",
    "def label_preprocessing(label: torch.Tensor):\n",
    "    int_label = torch.zeros(label.shape[1:])\n",
    "    for x, row in enumerate(label.permute(1, 2, 0)):\n",
    "        for y, color in enumerate(row):\n",
    "            int_label[x, y] = COLORMAP[tuple(color.tolist())]\n",
    "    \n",
    "    return int_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"ImageSets/Segmentation/train.txt\"\n",
    "IMAGE_PATH = \"JPEGImages\"\n",
    "SEGMENTATION_LABEL_PATH = \"SegmentationClass\"\n",
    "\n",
    "dataset_path = \"../../datasets/VOC2012\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 images processed.\n",
      "40 images processed.\n",
      "60 images processed.\n",
      "80 images processed.\n",
      "100 images processed.\n",
      "120 images processed.\n",
      "140 images processed.\n",
      "160 images processed.\n",
      "180 images processed.\n",
      "200 images processed.\n",
      "220 images processed.\n",
      "240 images processed.\n",
      "260 images processed.\n",
      "280 images processed.\n",
      "300 images processed.\n",
      "320 images processed.\n",
      "340 images processed.\n",
      "360 images processed.\n",
      "380 images processed.\n",
      "400 images processed.\n",
      "420 images processed.\n",
      "440 images processed.\n",
      "460 images processed.\n",
      "480 images processed.\n",
      "500 images processed.\n",
      "520 images processed.\n",
      "540 images processed.\n",
      "560 images processed.\n",
      "580 images processed.\n",
      "600 images processed.\n",
      "620 images processed.\n",
      "640 images processed.\n",
      "660 images processed.\n",
      "680 images processed.\n",
      "700 images processed.\n",
      "720 images processed.\n",
      "740 images processed.\n",
      "760 images processed.\n",
      "780 images processed.\n",
      "800 images processed.\n",
      "820 images processed.\n",
      "840 images processed.\n",
      "860 images processed.\n",
      "880 images processed.\n",
      "900 images processed.\n",
      "920 images processed.\n",
      "940 images processed.\n",
      "960 images processed.\n",
      "980 images processed.\n",
      "1000 images processed.\n",
      "1020 images processed.\n",
      "1040 images processed.\n",
      "1060 images processed.\n",
      "1080 images processed.\n",
      "1100 images processed.\n",
      "1120 images processed.\n",
      "1140 images processed.\n",
      "1160 images processed.\n",
      "1180 images processed.\n",
      "1200 images processed.\n",
      "1220 images processed.\n",
      "1240 images processed.\n",
      "1260 images processed.\n",
      "1280 images processed.\n",
      "1300 images processed.\n",
      "1320 images processed.\n",
      "1340 images processed.\n",
      "1360 images processed.\n",
      "1380 images processed.\n",
      "1400 images processed.\n",
      "1420 images processed.\n",
      "1440 images processed.\n",
      "1460 images processed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[196349993,\n",
       " 1780580,\n",
       " 758311,\n",
       " 2232247,\n",
       " 1514260,\n",
       " 1517186,\n",
       " 4375622,\n",
       " 3494749,\n",
       " 6752515,\n",
       " 2861091,\n",
       " 2060925,\n",
       " 3381632,\n",
       " 4344951,\n",
       " 2283739,\n",
       " 2888641,\n",
       " 11995853,\n",
       " 1670340,\n",
       " 2254463,\n",
       " 3612229,\n",
       " 3984238,\n",
       " 2349235]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dataset_path, TRAIN_PATH)) as f:\n",
    "    file_list = [string for string in f.read().split('\\n') if string]\n",
    "\n",
    "\n",
    "num_classes = [0] * 21\n",
    "\n",
    "for i, file in enumerate(file_list):\n",
    "    label = torchvision.io.read_image(\n",
    "        os.path.join(dataset_path, SEGMENTATION_LABEL_PATH, file + '.png'),\n",
    "        mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "    processed_label = label_preprocessing(label)\n",
    "    \n",
    "    for idx in range(21):\n",
    "        num_classes[idx] += int(torch.sum(processed_label == idx))\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"{i + 1} images processed.\")\n",
    "\n",
    "\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481059906394354"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes[0] / sum(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 15}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e322847a449c710f75ffc7b51d83d0cba99eb81db77ebc2acc1ec8be468b8df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
